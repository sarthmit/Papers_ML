# Papers_ML

A list of good and relevant papers to read.

### Theoretical Analysis of GANs

Improved Techniques for Training GANs

Towards Principled Methods for Training Generative Adversarial Networks

The Numerics of GANs

Which Training Methods for GANs do actually Converge?

### Convergence Speed of Over-parameterized models

On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization

### Generalization Study in Neural Networks

Generalization in Deep Learning

High-dimensional dynamics of generalization error in neural networks

In Search Of The Real Inductive Bias: On The Role Of Implicit Regularization In Deep Learning

Understanding Deep Learning Requires Rethinking Generalizaiton

Train longer, generalize better: closing the generalization gap in large batch training of neural networks

Towards Understanding the Role of Over-Parametrizationin Generalization of Neural Networks

SGD Learns Over-parameterized Networks That Provably Generalize On Linearly Separable Data

### Batch-Normalization

Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

How Does Batch Normalization Help Optimization?(No, It Is Not About Internal Covariate Shift)

### Variational Inference with Reparameterization

The Generalized Reparameterization Gradient

Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms

Implicit Reparameterization Gradients

### SGD as Variational Inference

Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles For Deep Networks
